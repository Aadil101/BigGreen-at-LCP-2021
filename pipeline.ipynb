{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('3.9.2': pyenv)",
   "metadata": {
    "interpreter": {
     "hash": "364b29872ddfcab1ec8c2399879a7f7be1d46986036ed92c25eb3e86b8ade17f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from macbook.lib.utils import preprocess, preprocess_for_bert, build_glove_embeddings, save_embeddings, save_other_features, log_transform\n",
    "from discovery.utils import load_features_stacked, load_feature_name_2_idx, get_sorted_mi\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/pickle/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /Users/pickle/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package cmudict to /Users/pickle/nltk_data...\n[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = 'cucumber'\n",
    "job_prev = 'pickle'\n",
    "path = './data/'+job\n",
    "if not os.path.isdir(path): os.mkdir(path)\n",
    "for subdirectory in ['macbook', 'discovery']:\n",
    "    path = './'+subdirectory+'/data/'+job\n",
    "    if subdirectory == 'discovery':\n",
    "        if not os.path.isdir(path+'/macbook'): os.mkdir(path+'/macbook')\n",
    "        if not os.path.isdir(path+'/colab'): os.mkdir(path+'/colab')\n",
    "for split in ['train', 'dev']:\n",
    "    path = './colab/new-mt-dnn/data_complex/lcp_'+split+'.tsv'\n",
    "    if not os.path.isfile(path):\n",
    "        dummy = pd.DataFrame([[0, 0.0, 'This is a pretty easy sentence.', 'easy']])\n",
    "        dummy.to_csv(path, sep='\\t', index=False, header=False, encoding='utf-8')"
   ]
  },
  {
   "source": [
    "# Load new data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([['This is a pretty easy sentence.', 'easy']], columns=['sentence', 'token'])\n",
    "mode = 'single'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "995it [00:00, 9948.52it/s]/Users/pickle/cs99/BigGreen-at-LCP-2021/macbook\n",
      "400000it [00:28, 14247.69it/s]\n",
      "Found 7(/9) words with w2v vectors\n",
      "Vocab size : 7\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.46it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 160.68it/s]/Users/pickle/cs99/BigGreen-at-LCP-2021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd ./macbook/\n",
    "preprocess(data, './data/'+job+'/'+mode+'_data_p.tsv')\n",
    "preprocess_for_bert(data, job, mode+'_data', do_round=False)\n",
    "config = {\n",
    "    'glove_path': './lib/glove/glove.6B.300d.txt',\n",
    "    'glove_lower': True,\n",
    "    'disambiguate': True,\n",
    "    'infersent_V': 1,\n",
    "    'infersent_MODEL_PATH': './lib/encoder/infersent%s.pkl',\n",
    "    'infersent_W2V_PATH': './lib/glove/glove.6B.300d.txt',\n",
    "    'infersent_lower': True,\n",
    "    'options_file': './lib/ELMo/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json',\n",
    "    'weight_file': './lib/ELMo/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5'\n",
    "}\n",
    "glove_embeddings = build_glove_embeddings(config)\n",
    "save_embeddings(data, glove_embeddings, config, './data/'+job+'/'+mode+'_data_d.tsv', multi=mode=='multi')\n",
    "save_other_features(data, None, config, './data/'+job+'/'+mode+'_data_o.tsv', multi=mode=='multi')\n",
    "log_transform(data, 'all', './data/'+job+'/'+mode+'_data_o.tsv')\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd discovery/\n",
    "# TODO:\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/pickle/cs99/BigGreen-at-LCP-2021/colab/new-mt-dnn\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "03/28/2021 05:53:43 Task lcp\n",
      "03/28/2021 05:53:43 data_complex/bert_base_cased/lcp_train.json\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "03/28/2021 05:53:43 data_complex/bert_base_cased/lcp_dev.json\n",
      "03/28/2021 05:53:43 data_complex/bert_base_cased/lcp_test.json\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Loaded 1 samples out of 1\n",
      "/Users/pickle/cs99/BigGreen-at-LCP-2021\n"
     ]
    }
   ],
   "source": [
    "!cp \"./macbook/data/{job}/{mode}_data_bert.tsv\" \"./colab/new-mt-dnn/data_complex/lcp_test.tsv\"\n",
    "!cp \"./macbook/data/{job}/{mode}_data_o.tsv\" \"./discovery/data/{job}/macbook/{mode}_data_o.tsv\"\n",
    "%cd ./colab/new-mt-dnn/\n",
    "!python prepro_std.py --model bert-base-cased --root_dir data_complex/ --task_def data_complex/lcp.yml\n",
    "if mode=='multi':\n",
    "    !python predict.py --task_def \"data_complex/lcp.yml\" --task lcp --task_id 0 --prep_input \"data_complex/bert_base_cased/lcp_test.json\" --score \"../../discovery/data/{job}/colab/{mode}_data_bert_scores.json\" --checkpoint \"checkpoints/bert-cased_lcp-single_2021-01-19T0332/model_3.pt\"\n",
    "else:\n",
    "    !python predict.py --task_def \"data_complex/lcp.yml\" --task lcp --task_id 0 --prep_input \"data_complex/bert_base_cased/lcp_test.json\" --score \"../../discovery/data/{job}/colab/{mode}_data_bert_scores.json\" --checkpoint \"checkpoints/bert-cased_lcp-single_2021-01-19T0309/model_4.pt\"\n",
    "%cd ../../"
   ]
  },
  {
   "source": [
    "# Train model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/pickle/cs99/BigGreen-at-LCP-2021/discovery\n",
      "[18:05:07] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[18:05:07] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "/Users/pickle/cs99/BigGreen-at-LCP-2021\n"
     ]
    }
   ],
   "source": [
    "%cd ./discovery/\n",
    "single_train = pd.read_csv('./data/'+job_prev+'/single_train_j.tsv', sep='\\t', index_col=0)\n",
    "X = load_features_stacked(job_prev, 'single_train')\n",
    "y = single_train['complexity'].to_numpy()\n",
    "single_train_feature_name_2_idx = load_feature_name_2_idx(job_prev, 'single_train')\n",
    "single_train_feature_idx_2_name = {idx: feature_name for feature_name, idx in single_train_feature_name_2_idx.items()}\n",
    "feature_names, mi = get_sorted_mi('./data/'+job_prev+'/single_train_mi.txt')\n",
    "picks = feature_names[:300]\n",
    "picks = [pick for pick in picks if X[:, single_train_feature_name_2_idx[pick]].std() != 0]\n",
    "col_idx = np.array([single_train_feature_name_2_idx[pick] for pick in picks])\n",
    "selector = VarianceThreshold(threshold=0.01)  # 0.1 indicates 99% of observations approximately\n",
    "_ = selector.fit(X[:, col_idx])  # fit finds the features with low variance\n",
    "picks = [pick for i, pick in enumerate(picks) if selector.get_support()[i]]\n",
    "col_idx = np.array([single_train_feature_name_2_idx[pick] for pick in picks])\n",
    "X_picks = X[:, col_idx]\n",
    "scaler = StandardScaler()\n",
    "X_picks_scaled = scaler.fit_transform(X_picks)\n",
    "model = XGBRegressor(colsample_bytree=0.7,learning_rate=0.03,max_depth=5,min_child_weight=4,n_estimators=225,nthread=4,objective='reg:linear',silent=1,subsample=0.7)\n",
    "model = model.fit(X_picks_scaled, y)\n",
    "%cd ../"
   ]
  },
  {
   "source": [
    "# Make predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}